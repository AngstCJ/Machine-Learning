# 吴恩达机器学习 #

## 第一章

### Machine learning algorithm:

- Supervised learning  (监督学习)

- Unsupervised learning  （无监督学习）
- Reinforcement learning (强化学习)
- recommender system (推荐系统)

### Supervised learning

"right answers" given  提前给定数据集，数据集中包含了正确答案

Regression: predict continuous valued output

Classification: discrete valued output (离散的输出)

### Unsupervised learning

聚类问题 cluster algorithms

find structure in data

## 第二章

### Linear regression with one variable

一元线性回归

training set -> learning algorithm -> h

h is one function    hypothesis 假设 机器学习中的术语 

### cost function

squared error cost function （平方误差函数） 回归最常用

### Gradient descent

梯度下降

起始点的选择可能会导致得出不同的局部最优解

<img src="D:\吴恩达计算器学习\2-1.jpg" alt="2-1" style="zoom: 50%;" />

需要同步更新

### Gradient descent intution

learning rate   学习速率

学习速率太大，可能会错过局部最优解，如果太小，下降的迭代次数太多

如果已经位于局部最优解，梯度下降将什么也不做，不会改变你的局部最优解

在逐渐到达局部最优解的过程中，由于导数越来越小，每次移动的步伐也越来越小，因此没有必要减小学习速率，在这个过程中。

### Gradient descent for linear regression

![2-2](D:\吴恩达计算器学习\2-2.jpg)

 Batch: Each step of gradient descent uses all the trainning examples

每一步梯度下降，都遍历了整个训练样本

梯度下降适用于大量的数据集

## 第三章

### 矩阵和向量

矩阵的维数 number of rows * number of columns  行数*列数

R^(4*2)  代表所有的4 * 2矩阵  以此类推 用R值某一特定维度的矩阵

vector 向量   n * 1 matrix n行1列的矩阵

### Addition and scalar multiplication

加法和标量乘法

矩阵加法：相应位置元素相加，相同维度的矩阵才可以相加

scalar: a real number 一个实数 矩阵所有元素逐一相乘

### Matrix-vector multiplication

矩阵和向量的乘法

### Matrix-matrix multiplication

### Matrix multiplication properties

矩阵的乘法特征：不可交换，具备结合律

### Inverse and transpose

逆和转置

只有方阵才会有逆矩阵

## 第四章

### Multiple features

多个特征变量

变量的符号说明

![4-1](D:\吴恩达计算器学习\4-1.jpg)

### Gradient descent for multiple variables

针对多变量的梯度下降

![4-2](D:\吴恩达计算器学习\4-2.jpg)

### Gradient descent in practice 1: Feature Scaling

特征缩放   确保特征值处在一个相似的范围

不同特征所处的范围如果相差较大，会导致梯度下降速度变慢，并且可能会来回波动

我们执行特征缩放时，通常会将特征的取值约束在-1到+1的范围之内

只要特征所处的范围比较小都行，不必要局限于-1 和 1

同样如果特征所处的范围非常小，也是不合适的

-3 到 +3 的范围都可以接受

-1/3 到 1/3 的范围可以接受，这是上下界

Mean normalization

均值归一化 replace x(i) with xi - ui

x = (x-u)/s  s是区间长度，也可以使用标准差

### Gradient descent in practise 2: Learning rate

学习速率

如果梯度下降算法正常工作，每一步迭代，都应该使代价函数下降

Example automatic convergence test:

Declare convergence if J(p) decreases by less than 10^(-3) in one iteration

告诉你是否已经收敛，当某次迭代前后，代价函数的变化小于10^(-3)

如果代价函数逐渐变大，或者代价函数上下波动，考虑使用较小的学习速率 （适用于线性回归）

0.003   0.03  0.3 3 ....

找到一个最小可能值，找到最大可能值，然后取最大可能值，或者比最大可能值略小的

### Features and polynomial regression

多项式回归

在使用高次多项式时，特征缩放就显得非常重要

选择合适的函数形式非常重要，不仅仅要拟合现有的数据，还要对数据集以外的数据有着合理的推测

特征选择

### Normal equation

正规方程

对于求解某些线性回归问题，这种方法会给我们更好的解

这是一种解析解法

![4-3](D:\吴恩达计算器学习\4-3.jpg)

证明过程如下

![4-4](D:\吴恩达计算器学习\4-4.jpg)

如果使用了正规方程，没有必要使用特征缩放

梯度下降算法：需要去选择学习速率，需要多次迭代，但是即使在特征非常多的情况下也能很好地工作

正规方程：尽管不需要去选择学习速率和多次迭代，但是需要去计算矩阵的逆，当我们谈论的特征数目非常多时，计算速度会变得非常慢。  一般以10000为界限

正规方程针对于少量特征线性回归提供了一种很好的解法，但是在很多情况下，我们并不会使用线性回归，而对于其他的回归算法，我们无法在使用正规方程

### Normal equation and non-invertibility (optional)

正规化矩阵不可逆可能的原因：

1. 特征冗余  某些特征之间存在确定的线性关系
2. 特征数目太多    可以考虑删除一些特征，或者使用正规化

使用pinv 函数  伪逆函数  即使针对不可逆矩阵也可以求出他的逆

## 第五章

### Basic operations

octave 基本操作

matlab 可以代替

## 第六章

### 分类

线性回归在很多分类问题都不再适用

在分类问题中，我们期待的输出应当是0或者1，但是当我们使用线性假设时，很容易就会超出这个范围

### Hypothesis Representation

假设陈述

Logistic Regression Model

期望输出在0-1之间

sigmod function g(x) = 1/(1 + e^(-x)) 

当x趋向于负无穷，函数趋向于0，当x趋向于正无穷，函数值趋向于1

在这里假设的输出，我们定义为其为1的概率

![6-1](D:\吴恩达计算器学习\6-1.jpg)

### Decision boundary

决策边界

因为当 z>=0 时，g(z) >= 0.5, 对应的 z = 0 这条分界线，被称为决策边界  

就是 g(z) = 0.5 的点的集合

决策边界是假设函数的属性，是由 theta 决定的

决策边界的形状有很多

### Cost function

针对logistic回归的代价函数

由于logistic回归中使用到了sigmod 函数，使得之前适用于线性回归的代价函数不再适用，原因在与 J（theta ） 变成了关于theta 的非凸函数，存在很多的局部最优解，梯度下降算法就无法保证其下降到全局最优解

![6-2](D:\吴恩达计算器学习\6-2.jpg)

因此我们需要找到一个新的代价函数

![6-3](D:\吴恩达计算器学习\6-3.jpg)

新的代价函数，在预测值完全背离时，也就是 pre_y + real_y = 1 时，代价都趋向于无穷大，当相等时，代价都为0

### Simplified cost function and gradient descent

简化的代价函数与梯度下降

可以将上述的两个等式合并

Cost(h(x),y) =  -ylog(h(x))-(1-y)log(1-h(x))

不难证明二者完全等价

梯度下降的偏导数推导

![6-4](D:\吴恩达计算器学习\6-4.jpg)

### Advanced optimization

高级优化

Conjugate gradient  共轭梯度法

BFGS

L-BFGS

使用上述三种算法，可以不用手动选择学习速率，一般来讲会比正常的梯度下降速度快的多，但是相应的，他们更加复杂

matlab中有相应的方法库

### Multi-class classification: One-vs-all

多元分类，一对多

选出最符合输入值的分类

![6-5](D:\吴恩达计算器学习\6-5.jpg)

## 第七章

### The problem of overfitting

过拟合问题

没有非常好的拟合训练数据 称为欠拟合 underfitting

过拟合 也被称作具有high variance 高方差

![7-1](D:\吴恩达计算器学习\7-1.jpg)

解决过拟合的方法

1. 减少特征，可以人工选择，也可以使用模型选择算法来自动决定使用哪些变量
2. 正则化，减少参数的大小

### Cost function

代价函数

在代价函数中加入惩罚项，来使某些参数非常小，简化模型，可以避免陷入过拟合问题

![7-2](D:\吴恩达计算器学习\7-2.jpg)

惩罚参数过大，会造成对参数的惩罚程度过大

### 线性回归的正则化

![7-3](D:\吴恩达计算器学习\7-3.jpg)

![7-4](D:\吴恩达计算器学习\7-4.jpg)

### logistic 回归的正则化

![7-5](D:\吴恩达计算器学习\7-5.jpg)

## 第八章 Neural Networks

### Non-linear hypotheses

非线性假设

当我们拥有的特征数量过多，那么他们的各种组合类别也无法计数，这种情况下继续使用logistic回归显然是不合适的，结果也容易过拟合

### Neurons and brain

神经元和大脑

神经网络算法，是为了模仿人的大脑所设计的算法

### Model representation

神经元从一些输入通路中接受信息，经过一些运算，将其传递给别的神经元

activation function 激活函数

神经网络就是一些神经元的组合

网络的第一层 input layer 输入层  

最后一层  output layer 输出层

中间层 hidden layer 隐藏层

![8-1](D:\吴恩达计算器学习\8-1.jpg)

神经网络计算的向量化

从输入层，经过隐藏层，再到输出层的过程叫做前向传播 forward propagation

![8-2](D:\吴恩达计算器学习\8-2.jpg)

network architectures  refers to how the different neurons are connected to each other

### Examples and intitutions 

例子与直觉理解

简单的逻辑运算利用神经网络实现

![8-3](D:\吴恩达计算器学习\8-3.jpg)

![8-4](D:\吴恩达计算器学习\8-4.jpg)

### Multi-class classification

多类别分类问题

N>2 个类别 输出层设置N个神经元，每个神经元的输出对应每种分类的概率，选取最大的

![8-5](D:\吴恩达计算器学习\8-5.jpg)

## 第九章 Neural Networks: Learning

### Cost function

代价函数

参照logist回归的代价函数，我们定义神经网络的代价函数如下

![9-1](D:\吴恩达计算器学习\9-1.jpg)

![9-2](D:\吴恩达计算器学习\9-2.jpg)

### Backpropagation algorithm

反向传播算法

<img src="D:\吴恩达计算器学习\9-6.jpg" alt="9-6" style="zoom:80%;" />

![9-3](D:\吴恩达计算器学习\9-7.jpg)

上述的是编程实现求解神经网络参数偏导得步骤，下面是个人的一点证明![9-3](D:\吴恩达计算器学习\9-3.jpg)

![](D:\吴恩达计算器学习\9-4.jpg)

![9-5](D:\吴恩达计算器学习\9-5.jpg)

这个偏差衡量的是，为了影响这些中间值，我们想要改变神经网络中的权重的程度

借用一下别的材料中对于反向传播中所出现的四个经典方程

![9-9.jpg](D:\吴恩达计算器学习\9-9.jpg.png)

### 展开参数

将所有的theta整合成一个长向量，作为初始theta向量。

在实现代价函数时，使用reshape函数重新得到theta矩阵

重点在于编程

### 梯度检测

我们使用公式推导的关于theta的偏导，另外我们也可以根据偏导数的定义，使用两侧逼近的方式计算出关于theta的偏导。在网络的反向传播过程中，我们需要时刻检测这两者是否近似相等  可以用于检测我们的反向传播是否正确

一次检验即可，正确了就是用自己的反向传播算法进行训练

使用两侧逼近的方式会很耗费时间，但是反向传播算法的运算速度很快

### 随机初始化

random initialisation

在网络训练时，如果说将所有的参数初始化为0，是不适用的

在更新完以后，针对每个隐藏层神经元，其对于前一层的输入的权重一直相等，某一层的神经元的激活值也一直相等。如此下去，神经网络并不能为我们训练出有效的函数

因此我们需要使用随机初始化

![9-8](D:\吴恩达计算器学习\9-8.jpg)

### Putting it together

组合到一起

训练神经网络的第一步

1. 选择一种网络架构   合理的默认选择是指选择一层神经元，如果使用了多层神经元，默认每个隐藏层的神经元个数相同
2. 训练神经网络  随机初始化权重-实现前向传播-实现代价函数的计算-实现反向传播过程计算偏导数
3. 使用梯度检测
4. 使用梯度下降算法或者其他方式计算出最优解  ps 神经网络的代价函数不是一个convex函数，因此可能会收敛到局部最优解

### Autonomous driving example

无人驾驶

## 第十章 Advice for applying machine learning

### Deciding what to try next

决定下一步做什么

在预测房价这个例子上，如果说已经拟合好了正则化的式子，但是在预测时仍然有较大的出入，下一步该做什么？

1. 得到更多的训练样本  但事实上数据没有想象中容易获取，改进的效果也不是人们所设想的
2. 减少特征 防止过拟合
3. 得到更多的特征
4. 添加多项式特征
5. 增加或减小正则化参数lambda的值

随机的选择上述几种方式很有可能会让我们花费了相应的事件，但是我们的模型却没有的到对应的提升

### Evaluating a hypothesis

评估我们的假设

一种防止过拟合和欠拟合的方式

首先将我们的数据集分成两部分，一部分称为trainning set 另一部分称为 test set 7:3

然后使用训练数据集学习参数，利用测试数据集计算误差

![10-1](D:\吴恩达计算器学习\10-1.jpg)

### Model selection and trainning/validation/testsets

模型选择和训练、验证、测试集

对于选择多项式的最大次数，我们可以分别比较其测试集误差的情况，选择测试集误差最小的theta，但是也存在一定的问题。相当于我们使用测试集拟合了一个新的参数d，也就是多项式的最高次数。这样的话测试集误差并不能代表这个假设的泛化能力。  ps:如果你有非常非常大量的数据集，可以使用划分成两部分的方式

为了解决上述问题，我们使用新的数据分割方式。

将数据分成三部分  training set    cross validation set(交叉验证集)    test set  6:2:2

### Diagnosing bias vs. variance

诊断偏差与方差

如果说模型表现得效果不好，那么可以归结为两类问题  高偏差与高方差 也就是过拟合和欠拟合的问题

随着多项式次数的逐渐增大，训练集误差会越来越小，甚至等于0

高偏差问题，训练误差，交叉验证误差都会很大  二者会很接近

高方差问题，训练误差会很小，交叉验证误差会很大

### Regularization and bias/variance

正则化和偏差方差

当lambda过大，对参数的惩罚程度过大，会出现欠拟合的问题，当lambda过小，对参数的惩罚程度过小，相当于没有使用正则化，又容易出现过拟合问题

与选择次数类似，用训练集拟合参数theta，用交叉验证集拟合lambda，最后用测试集评估模型的泛化能力

在这里训练误差，验证误差，测试误差都不包含正则化项

### Learning curve

学习曲线

当出现高偏差时，随着训练集数目的增多，验证误差会逐渐减小，训练误差会逐渐增大，最终二者会非常接近

因为出现高偏差时，即使随着数据样本数量的增大，拟合的曲线几乎不会有什么变化，验证集误差也就会趋于稳定

高偏差问题可以有很高的训练误差和很高的验证误差反映出来

如果说一个学习算法处于高偏差问题，即使增多数据个数，也不会有很大帮助



如果说一个算法处于高方差问题，即使随着样本数据的增大，训练误差会逐渐增大，验证误差会越来越小，但是二者之间仍然会有较大的差距。训练误差仍旧会处于一个较低的水平，验证误差会处于一个较高的水平

当处于高方差问题时，获取更多的训练数据是有帮助的

### Deciding what to try next

![10-2](D:\吴恩达计算器学习\10-2.jpg)

与神经网络的关联

当你的网络比较小，容易出现欠拟合问题，计算量也比较小

当网络结构比较大，容易出现过拟合问题，同时计算量会比较大

通常我们会选用大型网络，同时使用正则化来解决过拟合的问题

在层数的选择时，我们将数据集分割成训练集，验证集，测试集，像选择lambda一样，选择合适的隐藏层层数

## Machine learning system design

### Prioritizing what to work on: Spam classification example

确定执行的优先级

垃圾邮件的分类器，在构建特征向量时，我们可以构建一个含有100个单词的列表，然后使用100个0-1变量表示是否在这封邮件中出现过，以此作为数据的x

![11-1](D:\吴恩达计算器学习\11-1.jpg)

### 误差分析

在开发机器学习相关应用时的推荐步骤

我们可以从validation set 中找出错误分类的样本，进而分析我们算法的改进之处在哪里。这也正是我们首先实现一个较为简单的算法的原因。

在改进学习算法时，有一个单一规则的数值评价指标对于我们决定是否采取某样措施非常有帮助。  ps:在验证集上做，而不是测试集

### Error metrics for skewed classes

对于偏斜类的误差评估

偏斜类 正样本或者负样本的个数非常接近总体  skewed classes

如果说我们的样本数据出现了偏斜类问题，分类精确度作为衡量我们算法的标准并不科学

一种新的度量标准  Precision/Recall 查准率和召回率

True positive  y_act = 1; y_pre = 1

True negative y_act = 0; y _pre = 0

False positive  y_act = 0; y _pre = 1

False negative y_act = 1; y _pre = 0

Precision(查准率)   = True pos/(True pos + False pos)

Recall(召回率) = True pos/(True pos + False neg)

![11-2](D:\吴恩达计算器学习\11-2.jpg)

### Trading off precision and recall

精确率和召回率的平衡

我们提高预测的边界值，例如在h(x) >= 0.7时才预测这个人有癌症 这样的话，我们的算法就有高查准率，低召回率，因为我们将预测的边界值提高了，会抛弃掉一部分原本预测为患有癌症的数据

另一种情况下，我们需要尽可能避免错过癌症患者，宁可错杀，不可漏过的思想，降低边界值，相应的，会有low Precision   high recall

F1Score = 2*P*R/(P+R)

用上述式子去衡量你的算法性能 这个值会考虑一部分P和R的平均值，但是他也会给而这种非常低的值更高的权重

### Data for machine learning

机器学习数据

多参数保证低偏差，大量数据保证了方差小  二者结合也就是完美  ps 数据包含足够多的特征

## Suppurt Vector Machines

### Optimization objective

优化目标

支持向量机中优化函数和假设的变动如下

![12-1](D:\吴恩达计算器学习\12-1.jpg)

### Large Margin Intuition

直观上对大间隔的理解

![12-2](D:\吴恩达计算器学习\12-2.jpg)

当y = 1时，只有当z >= 0时，代价函数才为0； 相应的，当y = 0时，只有当z <= -1 时，代价函数才为0

相当于在SVM中，构建了一个安全间距。

SVM在分离数据时，会尽量用大的间距去分离，这也是SVM具有鲁棒性的原因，建立在C非常大的情况下

SVM对异常点会比较敏感

### The mathematics behind large margin classification

大间隔分类器的数学原理

推导之后找到更详细的附

### Kernels 1

核函数

选择三个标记点 l(1) l(2) l(3)

高斯核函数 exp(||x-l(1)||^2/(2*sigma))

当向量X与向量l非常接近时，核函数非常接近1  反之，核函数非常接近0

sigma值越低，下降的速度就会越快，反之，就会越慢

见下图![12-3](D:\吴恩达计算器学习\12-3.jpg)

![12-4](D:\吴恩达计算器学习\12-4.jpg)

针对上图给出的theta，我们可以得出，当样本点距离特征点1和特征点2非常近时，会预测为1，也就得到了左侧红线画出的不规则的决策边界

### Kernels 2

如何得到这些标记点

将训练集的样本直接作为标记点  如果说有m个样本，那么我们就有m个标记点

因此可将原来m*(n+1) 的输入 转换为 m*(m+1)  有一个特征始终为0  predict y = 1 if theta'*f >= 0

大部分的支持向量机最后一项不会使用theta'*theta 而是会使用 theta'*M*theta 其中M为某个矩阵，依赖于使用的核函数  主要是为了解决当我们的训练数据非常大时的计算花销问题

支持向量机中的参数C 

如果过大，容易导致低偏差，高方差  反之，容易导致 高偏差，低方差

sigma^2 如果过大，特征f比较平缓，容易导致高偏差，低方差  因为高斯核函数比较平缓，因此容易得到一个随着输入变化比较平缓的函数

### Using an SVM

尽管不需要自己去写求解SVM的软件，但是需要选择参数C和核函数Kernel

应用No kernel(linear kernel) 的场景  数据特征很多，数据集大小很小

应用高斯核函数情景：n很大，m很小  需要选择参数sigma^2

Note: 在使用高斯核函数之前进行特征缩放

所选用的核函数需要满足莫塞尔定理，只用这样在求解参数时才能用到大量的数值优化方法

其他的核函数：

1. 多项式核函数  （X‘*l+constant） ^ degree  通常用在X和l都是严格的非负数情况下  效果并不好
2. String kernel   chi-square kernel 卡方核函数    histogram intersection kernel 直方相交核函数

多分类的问题  很多SVM已经内置了多分类的函数  此外，也可以使用之前的 one vs all方法

![12-5](D:\吴恩达计算器学习\12-5.jpg)

在上述几种情况下，神经网络都能有不错的效果，但是其训练花费的事件比较长

## Clustering

### Unsupervised learning introduction

无监督学习介绍

在无监督学习中，我们数据不带有任何标签

聚类的应用：市场分割 社交网络分析  组织计算机群 天文数据分析

### K-means algorithm

K-means算法

两件事：簇分配 移动聚类中心

移动聚类中心时的步骤，将现在的聚类中心移动到现在属于该类的点的平均值处

![13-1](D:\吴恩达计算器学习\13-1.jpg)

### Optimization objective

优化目标

![13-2](D:\吴恩达计算器学习\13-2.jpg)

观察K-means的两个步骤  第一步是在聚类中心不变的情况下，改变每个点所属的簇，最小化代价函数，第二步是在每个点所属簇不变的情况下，更改聚类中心的位置，来最小化代价函数

### Random initialization

随机初始化

一种有效的随机初始化方法，在样本中随机选取K个，以此作为K个聚类中心的初始值

为了防止落到局部最优解，我们可以不仅初始化一次

如果我们的K比较小，在2-10之间，多次初始化会帮助我们找到一个不错的局部最优，但是如果我们的K比较大，多次随机初始化并不会有什么明显改善

![13-3](D:\吴恩达计算器学习\13-3.jpg)

### Choosing the number of clusters

选择聚类数量

根据数据可视化，选择聚类的数量总是模棱两可的的

Elbow method（肘部法则）:选择图像“肘部”的K![13-4](D:\吴恩达计算器学习\13-4.jpg)

有一定作用，但不能期待他时刻有用，因为我们很有可能找不到所谓的"elbow"，如上图右所示

## Dimensionality Reduction 降维

### Motivation 1: Data Compression

目标1 数据压缩

将原本相关性很强的数据用一个数据表示，就得到了原始数据的一个非常不错的近似，与此同时，对于内存的需求也会相应的降低

eg: 将二维平面上的点投影到一条直线上   将三维空间上的点投影到某个平面上

### Motivation 2: Data Visualization

目标2 可视化

针对一组高维的数据，利用压缩降维使这些数据可以被展示出来，可以帮助我们迅速发现数据本身的某些特征

### Principal Component Analysis problem formulation

主成分分析

PCA就是找到一个低维平面，使得这些数据投影到低维平面的距离平方和最小

在使用PCA之前，最常用的是使用均值归一化和特征规范化

![14-1](D:\吴恩达计算器学习\14-1.jpg)

注意区分线性回归和PCA的区别

![14-2](D:\吴恩达计算器学习\14-2.jpg)

其一是最小化的目标不同，其二在PCA中每个数据点都是平等对待的

### Principal Component Analysis algorithm

主成分分析算法

第一步对数据的预处理  均值标准化 如果不同特征的范围不同 还需要进行特征缩放

PCA要做的事情：找到相应的低维向量  确定在低维向量中的表示

![14-3](D:\吴恩达计算器学习\14-3.jpg)

### Choosing the number of principal components

主成分数量选择

![14-4](D:\吴恩达计算器学习\14-4.jpg)

百分之99的方差被保留下来了

不等式右侧如果是0.05 对应的就是百分之九十五的方差被保留下来了

不小于85的值都是比较典型的  常用95-99

很多数据特征相关，降维依然可以保留95以上的特征

算法实现的过程如下：从K=1开始，检验不等式是否成立，如果不成立，使K+1，重复上述步骤

但是上述迭代计算消耗很大

更为有效的方式如下

![14-5](D:\吴恩达计算器学习\14-5.jpg)

### Reconstruction from compressed representation

压缩重现

X_approx = U_reduce * z

### Advice for applying PCA

应用PCA的建议

PCA对监督学习的加速

1. 抽取出数据集中x
2. 应用PCA对其降维得到z
3. 将(z,y)组成的数据集通过学习算法进行训练

需要注意的是，我们降维处理时所需要的中间量都是在训练集上得到的，这个映射也可以很好的使用在交叉验证集和测试集上

错误使用：利用PCA防止过拟合

在使用PCA之前，先使用raw data去做我们要做的事情，如果说效果不理想，我们在考虑使用PCA

## Anomaly Detection 异常检测

### Problem motivation

问题动机

我们有了一系列飞机引擎特征的无标签数据，在某一天，我们得到了一个新的飞机引擎的特征数据，我们需要判断其是否合格

更一般化的描述，给定一系列特征数据，我们需要判定一个新的数据是否Anomalous

常用做法是建立训练集的概率模型p(x)，当P(x_test) < threhold  认为Anomalous

![15-1](D:\吴恩达计算器学习\15-1.jpg)

### Gaussian distribution

高斯分布

sigma越小，概率密度函数瘦高

参数估计：给定一组数据集，我们估计其u和sigma  通常我们u取平均值，sigma^2取方差

### Algorithm

单点的概率密度函数值其实就代表了该点的概率

![15-2](D:\吴恩达计算器学习\15-2.jpg)

### Developing and evaluating an anomaly detection system

开发和评估异常检测系统

同样的，需要有一个实数去评估我们的算法性能

假设我们有一些带有标签的数据；在训练集上fit我们的参数之后，在交叉验证集和测试集上验证性能

训练集上虽然我们把它当作无标签数据处理，但是其内部应当绝大部分是正常产品

数据有明显的偏斜性，因为绝大部分应该是正常产品，因此我们需要使用查准率和召回率以及F1Score来作为相应的评价指标

使用交叉验证集去fit episilion

![15-3](D:\吴恩达计算器学习\15-3.jpg)

### Anomaly detection vs. supervised learning

异常检测VS监督学习

使用二者的情景

当我们positive example数量非常少0-20  negativate example数量很多  我们使用异常检测

当两种数量都非常大时，我们使用监督学习

在工业部件中，由于损伤可能有多种，但是positive example数量很少，监督学习很难从这些数据中学习到异常的特征，而且还会有许多新的特征

### Choosing what features to use

选择要使用的特征

当我们数据特征与高斯分布的差异较大时，即使算法也能够正常运行，但是我们可以对数据进行相应的处理，让我们的算法更加高效

像平方根，取对数，取次方等等都是可以使用的转换方式

误差分析时，类似监督学习的过程，我们找到那些错误归类的样品，我们希望的是p足够大，针对于正常样本，看看能否启发我们找到一些新的特征

选择特征时会选择那些通常不会特别特别大，也不会特别特别小的特征

### 多变量高斯分布

不再为一个特征单独建模，相反为一整个特征向量建模

和单变量的高斯分布类似，sigma^2越小，图像越瘦高

优点是考虑到了变量之间的相关性

### Anomaly detection using the multivariable Gaussian distribution

使用多变量高斯分布的异常检测

![15-4](D:\吴恩达计算器学习\15-4.jpg)

简单来讲，分为两步，首先使用数据集拟合参数u和sigma，接着对于一个全新的数据，进行相应的预测

当多元高斯分布的协方差矩阵，除了对角线以外的元素都是0的时候，就是之前的高斯分布模型

如何在两种模型选择

原始模型：人为的选择一些特征的一个组合，通过捕捉这些异常组合来进行缺陷检测  增加一个新的特征，这个新的特征就是这些特征的组合方式  计算成本非常低

多元高斯模型：自动地考虑了特征之间的相关性  必须限制m > n   可以在m 远大于n的时候使用多元高斯分布

原始模型用的会更多一点

在使用多元高斯模型sigma矩阵不可逆的原因：

1. 没有满足m > n
2. 存在特征冗余

## Recommender Systems  推荐系统

### Problem formulation

问题规划

特征学习的思想

一些符号定义

![16-1](D:\吴恩达计算器学习\16-1.jpg)

### Content-based recommendations

基于内容的推荐算法

每部电影可以用一个特征向量，包含有固定的输入1和表示爱情和动作成分的两个量

针对每个用户j，我们可以用一组参数去预测对于某部电影的评分

与线性回归不同的是，优化的目标函数有细微的差别

优化的目标与相应的梯度下降步骤如下

![16-2](D:\吴恩达计算器学习\16-2.jpg)

![16-3](D:\吴恩达计算器学习\16-3.jpg)

但是很多情况下，我们并没有这样的特征变量x去描述一部电影

### Collaborative filtering

协同过滤

假定每个用户有一个向量可以表示对每种电影的喜欢程度

我们的问题变成了，每个用户都有一个向量来表示其对各种电影的喜欢程度，如何找到每部电影的特征向量，使用户的评分接近预估的分数

所以我们的优化目标就有所改变

![16-4](D:\吴恩达计算器学习\16-4.jpg)

协同过滤如下![16-5](D:\吴恩达计算器学习\16-5.jpg)

我们一般的做法时，随机给定一些初始的theta，然后重复迭代上述过程多次

### Collaborative filtering algorithm

协同过滤算法

我们对于协同过滤更高效的做法是将两个目标函数结合到一起

需要注意的是，在这里theta 和 x 都是n维的，不再有之前的x_0项，因为我们现在是在学习所有的特征

![16-6](D:\吴恩达计算器学习\16-6.jpg)

具体的算法步骤如下

![16-7](D:\吴恩达计算器学习\16-7.jpg)

上述建立的前提是，每部电影被多人评价，每个人评价了多部电影

### Vectorization: Low rank matrix factorization

向量化：低秩矩阵分解

`predicted_ratings = X*theta';`

这也叫做低秩矩阵分解

找到相关的电影

||x_i - x_j||

可以用来衡量两部电影的相关性  我们可以向用户推荐该值最小的几部电影

### Implementational detail: Mean normalization

实现细节：均值归一化

当一个用户还没有评价任何一部电影，那么我们对他所计算出的theta将会是一个0向量，相应的，对所有电影的预测分数也会是0

步骤如下

1. 将所有被评价过的电影的分数减去评分的平均值，未被评价的保持不变
2. 利用步骤1得到的数据集进行学习得到参数theta
3. 预测的分数 为 Theta'*X + u

## Large scale machine learning 大规模机器学习

### Learning with large datasets

学习大数据集

在机器学习中，决定因素往往不是好的算法，而是谁拥有大量的数据

在面对极其大量的数据时，计算梯度下降我们可以只用其的一个一个子集![17-1](D:\吴恩达计算器学习\17-1.jpg)

如上图，即使我们将m增加到1000000000，也不见得算法会有很大效率的提升

### Stochastic gradient descent

随机梯度下降

传统的梯度下降算法在数据集非常大的时候消耗的计算资源太多

传统的梯度下降 也称作  batch gradient descent 批量梯度下降

随即梯度下降，遍历所有的样本，每次只选用一组数据对所有参数进行更新

最外层循环通常1-10次即可  取决于你的数据集大小

![17-2](D:\吴恩达计算器学习\17-2.jpg)

### Mini-batch gradient descent

Mini-batch 梯度下降

Mini-batch梯度下降介于随机梯度下降和batch梯度下降之间，每次更新用到b个样本，而不是m个

b = mini-batch size   2-100

比起随机梯度下降，mini-batch梯度下降更能保证沿着代价函数下降最快的方向，也就是梯度方向

![17-3](D:\吴恩达计算器学习\17-3.jpg)

### Stochastic gradient descent convergence

随机梯度下降收敛

1. 在更新theta前计算代价函数的值
2. 每经过1000次迭代，画出代价函数关于迭代次数的图像

在画出图像时，代价函数是几个样本的平均值容易造成图像的噪声，因此，选择一个合适的数据集大小很有必要

如果我们想让算法收敛，就是随迭代次数减小学习速率，但是很多人不会选择使用

### Online learning

在线学习

我们重复的过程

1. 得到一个对应一个用户的数据（x,y）
2. 根据这个新的数据，对参数更新
3. 丢弃这个样本

这种算法可以适应用户的偏好变化

上述每次更新的时候得到的可能不止一个样本，也有可能是10个

### Map-reduce and data parallelism

减少映射和数据并行

我们可以将训练集均分成k份，然后分给k个计算机计算代价函数的和，最后由中心服务器整合更新参数

如此应用的前提是，我们使用的学习算法可以表示成在训练集上的函数求和问题

在一台机器上，也可以把数据分给相应的core来完成相似的功能，这样就避免了通信传输的问题

![17-4](D:\吴恩达计算器学习\17-4.jpg)

## Application example: Photo OCR

### Problem description and pipeline

问题描述

The Photo OCR problem 照片光学字符识别问题

实现OCR步骤

1. Text detection 文本探测
2. Character segmentation 字符分割
3. Character classification 字符分类

这样的一个系统称为machine learning pipeline  机器学习流水线

### Sliding windows

滑动窗口

在进行行人检测时，每个行人的长宽比都是大致不变的，因此我们可以用一个固定大小的图片的训练集去得到一个模型判断是否存在行人，紧接着在更大的图片上，我们一次滑动我们所选的图片块，图块每次移动的距离叫做步长step_size  我们也可以相应的调整图块的大小，得到更好的效果。

步长越短越精准，但是相对的更加耗时

相似地，我们可以训练一个分类器判断一个图块中是否包含文字，然后利用滑动窗口的思想，当判断有文字的区域集中于某处时，就可以认为包含文字了

下一步，我们训练另外一个分类器，判断图块中是否存在图像分割的地方

### Getting lots of data: Artificial data synthesis

获取大量数据 人工数据合成

一个有效的得到高性能机器学习系统的方法：使用一个低偏差的机器学习算法，并且使用庞大的数据集训练他

人工数据合成两种方法

1. 自己创造数据
2. 从已有的训练数据集，通过某种方式扩充训练集

可以通过引入失真数据来扩充数据集  例如图片文件的拉伸，音频文件添加噪音等等

通常在数据中加入完全随机，毫无意义的噪音不会有很大帮助

在花费精力生成数据之前，确保自己的分类器有较低的偏差

### Celling analysis: What part of pipeline to work next

上限分析，假设在pipeline 的某个阶段我们人工给了他完全正确的结果，观察系统整体的准确率有多大提升，由此来决定我们将时间花费在哪一个模块

# 完结撒花

